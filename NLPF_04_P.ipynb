{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPF_04_P.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqTGaftBZTTEi4KB8ggQsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satuelisa/NLPF/blob/main/NLPF_04_P.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOzz3PQlEYpi",
        "outputId": "37f9a473-008f-478d-e684-8fd606f04eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gutenbergpy in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (3.0.4)\n",
            "Requirement already satisfied: httpsproxy-urllib2 in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (1.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (57.4.0)\n",
            "Requirement already satisfied: lxml>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (4.2.6)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from gutenbergpy) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gutenbergpy # do not redo this if on your own computer, we already used it in Session 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gutenbergpy.textget # let's reuse what we did in Session 1 to get some example text\n",
        "target = 2701 # this one is Herman Melville's Moby Dick\n",
        "raw  = gutenbergpy.textget.get_text_by_id(target) # content\n",
        "text = gutenbergpy.textget.strip_headers(raw) # remove header\n",
        "s = text.decode(\"utf-8\") # get a string from the byte sequence\n",
        "content = s[s.rindex('CHAPTER 1. Loomings'):] # using the LAST time the string appears since it is rindex\n",
        "import re \n",
        "nodigits = re.sub(r'[0-9]+', '', content)\n",
        "clean = re.compile(r'\\s+') \n",
        "ok = clean.sub(' ', nodigits)\n",
        "potential = ok.split('CHAPTER ')\n",
        "stripped = [ candidate.strip().lstrip() for candidate in potential ] # leading and trailing space \n",
        "chapter = [ s for s in stripped if len(s) > 0 ] # keep only the ones with content\n",
        "print(len(chapter), ' chapters')\n",
        "print(chapter[0][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Snl2ZvnJdEG",
        "outputId": "d4a6bf7a-f690-4352-cda1-1fc867aba68f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149  chapters\n",
            ". Loomings. Call me Ishmael. Some years ago—never mind how long precisely—having little or no money \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, time to tag the parts of speech."
      ],
      "metadata": {
        "id": "unNKmPj3L8DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # we need punctuation specs in order to tokenize\n",
        "nltk.download('averaged_perceptron_tagger') # and the tagger, too (just do these once per computer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hImLd45MLeW",
        "outputId": "9ef6ae38-033d-4260-c586-4d339b8d9cfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(chapter[0])\n",
        "tags = nltk.pos_tag(tokens)\n",
        "print(tags[:12])\n",
        "for special in ['hand', 'stream', 'deliberate', 'bathed', 'peep']:\n",
        "  print([(word, tag) for (word, tag) in tags if word == special])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Enkx3C1OP3o",
        "outputId": "fb074db6-ff38-4de1-f034-471c9ecf80d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', '.'), ('Loomings', 'NNS'), ('.', '.'), ('Call', 'VB'), ('me', 'PRP'), ('Ishmael', 'NNP'), ('.', '.'), ('Some', 'DT'), ('years', 'NNS'), ('ago—never', 'RB'), ('mind', 'VB'), ('how', 'WRB')]\n",
            "[('hand', 'NN'), ('hand', 'NN'), ('hand', 'NN')]\n",
            "[('stream', 'NN'), ('stream', 'NN')]\n",
            "[('deliberate', 'VB')]\n",
            "[('bathed', 'VBN')]\n",
            "[('peep', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partial list of what those tag acronyms mean is available in [Table 5-1 of the Python textbook](https://learning.oreilly.com/library/view/natural-language-processing/9780596803346/ch05s02.html#tab-simplified-tagset).\n",
        "\n",
        "Note that we know have info on which words are names, such as *Ishmael* which got tagged to NNP.\n",
        "\n",
        "So, how could the tagger know that a word like *call* is a verb here instead of a noun? From *context*: what other words surround it. The concept of \"other surrounding words\" brings us to *bigrams* (two-word sequences) and *n* grams in general. Chapter 5 of the Python textbook discusses how to train those.\n",
        "\n",
        "The state-of-the art technology to deal with content in tagging are *transformers*, one example of which is the *Brill tagger*. Let's try that.\n"
      ],
      "metadata": {
        "id": "_pWP0U0VR9gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.tag\n",
        "from nltk.tag import brill # documentation at https://www.nltk.org/api/nltk.tag.brill.html\n",
        "import nltk.corpus, itertools\n",
        "nltk.download('brown') # an example corpus, just download once on your machine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdFUEeC2U41J",
        "outputId": "74e40417-db30-434b-a68e-bcae5b0d4d3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now combine elements of Chapter 5 of the Python textbook with those from [an online tutorial](https://www.geeksforgeeks.org/nlp-brill-tagger/) to build an example. Also [Jacob's blog](https://streamhacker.com/2008/11/03/part-of-speech-tagging-with-nltk-part-1/) may be an informative additional source of information, although not all of the code fragments in the tutorial or the blog are self-sufficient and/or compatible with the latest versions of the libraries involved."
      ],
      "metadata": {
        "id": "2LqDn4TVa2wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import brill, brill_trainer\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# specify a model to train\n",
        "init = DefaultTagger('NN') # a default starting point\n",
        "templates = nltk.tag.brill.nltkdemo18() # use the demo templates\n",
        "trainer = brill_trainer.BrillTaggerTrainer(init, templates)\n",
        "data = brown.tagged_sents(categories = 'news') # some training data\n",
        "n = len(data)\n",
        "sample = int(n * 0.1) # the smaller the faster (but of course also worse in general)\n",
        "print(n, '->', sample)\n",
        "tiny = 5 # just a few rules so that this will not take too long\n",
        "tagger = trainer.train(data[:sample], max_rules = tiny) # train the model with the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENpx08v6ZsB0",
        "outputId": "0aa20597-7829-4ff0-f55c-a421c58143fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4623 -> 462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out (tweak the proportion in `sample` and the number of rules in `tiny` for improved results albeit with increased training time."
      ],
      "metadata": {
        "id": "tQcXSDDRjdAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ttags = tagger.tag(tokens)\n",
        "part = 15\n",
        "print('NEW', ttags[:part])\n",
        "print('OLD', tags[:part])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsuVn-00jkrW",
        "outputId": "96d8986f-7ff7-42d7-aaa2-ebdb438f89c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEW [('.', 'NN'), ('Loomings', 'NN'), ('.', 'NN'), ('Call', 'NN'), ('me', 'NN'), ('Ishmael', 'NN'), ('.', 'NN'), ('Some', 'NN'), ('years', 'NN'), ('ago—never', 'NN'), ('mind', 'NN'), ('how', 'NN'), ('long', 'NN'), ('precisely—having', 'NN'), ('little', 'NN')]\n",
            "OLD [('.', '.'), ('Loomings', 'NNS'), ('.', '.'), ('Call', 'VB'), ('me', 'PRP'), ('Ishmael', 'NNP'), ('.', '.'), ('Some', 'DT'), ('years', 'NNS'), ('ago—never', 'RB'), ('mind', 'VB'), ('how', 'WRB'), ('long', 'JJ'), ('precisely—having', 'JJ'), ('little', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the results are clearly different. The newer tagger knows that *Call* is in fact a verb in this context instead of a noun. The same applies to *mind*."
      ],
      "metadata": {
        "id": "8fjiH0j4j-y4"
      }
    }
  ]
}